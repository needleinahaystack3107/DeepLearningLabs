{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"QqeXlHhR1fV9"},"outputs":[],"source":["import pandas as pd\n","import tensorflow as tf"]},{"cell_type":"markdown","metadata":{"id":"gMTOdU0Z206R"},"source":["Basic Dense Layer Perceptron "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dckW5TnU1tO_"},"outputs":[],"source":["class MyDenseLayer(tf.keras.layers.layer):\n","  def _init_(self,input_dim,output_dim):\n","    super(MyDenseLayer, self)._init_()\n","\n","    ## Initialize weights and bias\n","    self.W = self.add_weight([input_dim,output_dim])\n","    self.b = self.add_weight([1,output_dim])\n","\n","  def call(self,inputs):\n","    ## Forward propagate the inputs\n","    z = tf.matmul(inputs,self.W) + self.b\n","\n","    ##Feed output through non linear activation function\n","    output = tf.math.sigmoid(z)\n","\n","    return output"]},{"cell_type":"markdown","metadata":{"id":"RQNXngCy3ERn"},"source":["Tensorflow has this class actually implemented already. We can just call it using tf.keras.layers.Dense(units=2). Units here refers to number of outputs."]},{"cell_type":"markdown","metadata":{"id":"cJT4wWWw5N40"},"source":["Sequential Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3rX9QTpK5QJ8"},"outputs":[],"source":["## Single Layer Neural Network\n","model = tf.keras.Sequential([\n","                             tf.keras.layers.Dense(4), ##n=4 here refers to the intermerdiate neuron count.\n","                             tf.keras.layers.Dense(2) ## 2 final outputs.\n","])\n","tf.keras.Sequential"]},{"cell_type":"markdown","metadata":{"id":"DEgFHPj19fFp"},"source":["Computing Loss of a Neural Network using Cross Entropy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMUot-n69vC1"},"outputs":[],"source":["loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y,predicted))"]},{"cell_type":"markdown","metadata":{"id":"HG4nEMkDIV3s"},"source":["Gradient Descent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xphRTAr59znI"},"outputs":[],"source":["weights = tf.Variable([tf.random.normal()])\n","\n","while True:\n","  with tf.GradientTape() as g:\n","    loss = compute_loss(weights)\n","    gradient = g.gradient(loss,weights)\n","\n","    weights = weights - lr*gradient"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hEa_JgIDMqG_"},"outputs":[],"source":["### Different types of gradient descent optimizers\n","tf.keras.optimizers.SGD ##Stochastic Gradient Descent\n","tf.keras.optimizers.Adam ##Adam\n","tf.keras.optimizers.Adadelta ##AdaDelta\n","tf.keras.optimizers.Adagrad ## Adagrad\n","tf.keras.optimizers.RMSProp ##RMSProp\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VTvSeNEbSDkJ"},"source":["Regularization in NN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UG1ODrTJSFhg"},"outputs":[],"source":["## Method 1 Dropout\n","tf.keras.layers.Dropout(p=0.5)\n","\n","## Everytime a different 50% of neurons in the dense/hidden layers are dropped from training\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPUGyuAN4e2jdbjWstJj8v7","collapsed_sections":[],"name":"Lecture 1 Notes: MIT_Course_TensorFlow_Basics.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.9.7 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.7"},"vscode":{"interpreter":{"hash":"b8ffe1c3664f4da60e7310446699d2db577a48881ad6e046ce2077bbfacdc2a3"}}},"nbformat":4,"nbformat_minor":0}
