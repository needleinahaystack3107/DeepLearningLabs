{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lecture 1 Notes: MIT_Course_TensorFlow_Basics.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPUGyuAN4e2jdbjWstJj8v7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"QqeXlHhR1fV9"},"outputs":[],"source":["import pandas as pd\n","import tensorflow as tf"]},{"cell_type":"markdown","source":["Basic Dense Layer Perceptron "],"metadata":{"id":"gMTOdU0Z206R"}},{"cell_type":"code","source":["class MyDenseLayer(tf.keras.layers.layer):\n","  def _init_(self,input_dim,output_dim):\n","    super(MyDenseLayer, self)._init_()\n","\n","    ## Initialize weights and bias\n","    self.W = self.add_weight([input_dim,output_dim])\n","    self.b = self.add_weight([1,output_dim])\n","\n","  def call(self,inputs):\n","    ## Forward propagate the inputs\n","    z = tf.matmul(inputs,self.W) + self.b\n","\n","    ##Feed output through non linear activation function\n","    output = tf.math.sigmoid(z)\n","\n","    return output"],"metadata":{"id":"dckW5TnU1tO_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tensorflow has this class actually implemented already. We can just call it using tf.keras.layers.Dense(units=2). Units here refers to number of outputs."],"metadata":{"id":"RQNXngCy3ERn"}},{"cell_type":"markdown","source":["Sequential Model"],"metadata":{"id":"cJT4wWWw5N40"}},{"cell_type":"code","source":["## Single Layer Neural Network\n","model = tf.keras.Sequential([\n","                             tf.keras.layers.Dense(4), ##n=4 here refers to the intermerdiate neuron count.\n","                             tf.keras.layers.Dense(2) ## 2 final outputs.\n","])\n","tf.keras.Sequential"],"metadata":{"id":"3rX9QTpK5QJ8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Computing Loss of a Neural Network using Cross Entropy"],"metadata":{"id":"DEgFHPj19fFp"}},{"cell_type":"code","source":["loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y,predicted))"],"metadata":{"id":"YMUot-n69vC1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Gradient Descent"],"metadata":{"id":"HG4nEMkDIV3s"}},{"cell_type":"code","source":["weights = tf.Variable([tf.random.normal()])\n","\n","while True:\n","  with tf.GradientTape() as g:\n","    loss = compute_loss(weights)\n","    gradient = g.gradient(loss,weights)\n","\n","    weights = weights - lr*gradient"],"metadata":{"id":"xphRTAr59znI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Different types of gradient descent optimizers\n","tf.keras.optimizers.SGD ##Stochastic Gradient Descent\n","tf.keras.optimizers.Adam ##Adam\n","tf.keras.optimizers.Adadelta ##AdaDelta\n","tf.keras.optimizers.Adagrad ## Adagrad\n","tf.keras.optimizers.RMSProp ##RMSProp\n","\n"],"metadata":{"id":"hEa_JgIDMqG_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Regularization in NN"],"metadata":{"id":"VTvSeNEbSDkJ"}},{"cell_type":"code","source":["## Method 1 Dropout\n","tf.keras.layers.Dropout(p=0.5)\n","\n","## Everytime a different 50% of neurons in the dense/hidden layers are dropped from training\n"],"metadata":{"id":"UG1ODrTJSFhg"},"execution_count":null,"outputs":[]}]}