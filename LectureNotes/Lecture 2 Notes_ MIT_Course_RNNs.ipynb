{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lecture 2 Notes: MIT_Course_RNNs.ipynb","provenance":[],"authorship_tag":"ABX9TyPqT3X/9hBHYDIFRtzqq2uy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["RNN Class Implementation"],"metadata":{"id":"4350CgLh-x11"}},{"cell_type":"code","source":["import pandas as pd\n","import tensorflow as tf"],"metadata":{"id":"9iBZmgF9BPDb","executionInfo":{"status":"ok","timestamp":1655055920252,"user_tz":240,"elapsed":5983,"user":{"displayName":"Abhijit Haridas","userId":"01170217024346627659"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class MyRNNCell(tf.keras.layers.Layer):\n","  def __init__(self,rnn_units,input_dim,output_dim):\n","    super(MyRNNCell,self).__init__()\n","\n","    ## Initialize weight matrices\n","    self.W_xh = self.add_weight([rnn_units,input_dim])\n","    self.W_hh = self.add_weight([rnn_units,rnn_units])\n","    self.W_hy = self.add_weight([output_dim,rnn_units])\n","\n","    ## Initialize hidden weights to zeros\n","    self.h = tf.zeros([rnn_units,1])\n","\n","  ## Forward Pass\n","  def call(self,x):\n","    ## Update hidden state\n","    self.h = tf.math.tanh(self.W_hh*self.h + self.W_xh*x)\n","    ## tanh is kind of a non linearity used to update hidden states\n","    \n","    ## Comput output\n","    output = self.W_hy*self.h\n","\n","    ## Return output and hidden state\n","    return output,self.h\n","\n","  ## Question: Where do we use the activation function here?\n","\n","\n","\n"],"metadata":{"id":"pFVAVl5s-9fI","executionInfo":{"status":"ok","timestamp":1655056173797,"user_tz":240,"elapsed":454,"user":{"displayName":"Abhijit Haridas","userId":"01170217024346627659"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["## RNN Implementation in tensorflow\n","tf.keras.layers.SimpleRNN(rnn_units)"],"metadata":{"id":"yK6e9sV2BRXV"},"execution_count":null,"outputs":[]}]}